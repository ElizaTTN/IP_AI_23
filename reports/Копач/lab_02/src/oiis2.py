import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from torchvision.models import resnet34, ResNet34_Weights
import requests
from PIL import Image
import io
import time
from tqdm import tqdm

# 1. –ù–ê–°–¢–†–û–ô–ö–ê –£–°–¢–†–û–ô–°–¢–í–ê –ò –ü–ê–†–ê–ú–ï–¢–†–û–í
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –õ–† ‚Ññ1 (Adam –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä)
batch_size = 64
learning_rate = 0.001
num_epochs = 10

# 2. –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –î–ê–ù–ù–´–• –î–õ–Ø Fashion-MNIST
# –î–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π ResNet34 (3 –∫–∞–Ω–∞–ª–∞)
transform_resnet = transforms.Compose([
    transforms.Resize((224, 224)),  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è ResNet
    transforms.Grayscale(num_output_channels=3),  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ 3 –∫–∞–Ω–∞–ª–∞
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
])

# –î–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π CNN (1 –∫–∞–Ω–∞–ª)
transform_custom = transforms.Compose([
    transforms.Resize((32, 32)),  # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π CNN
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è 1 –∫–∞–Ω–∞–ª–∞
])

# 3. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
print("–ó–∞–≥—Ä—É–∑–∫–∞ Fashion-MNIST...")
train_dataset_resnet = torchvision.datasets.FashionMNIST(
    root='./data', train=True, download=True, transform=transform_resnet)
test_dataset_resnet = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=True, transform=transform_resnet)

train_dataset_custom = torchvision.datasets.FashionMNIST(
    root='./data', train=True, download=True, transform=transform_custom)
test_dataset_custom = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=True, transform=transform_custom)

train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=batch_size, shuffle=True)
test_loader_resnet = DataLoader(test_dataset_resnet, batch_size=batch_size, shuffle=False)

train_loader_custom = DataLoader(train_dataset_custom, batch_size=batch_size, shuffle=True)
test_loader_custom = DataLoader(test_dataset_custom, batch_size=batch_size, shuffle=False)

# –ö–ª–∞—Å—Å—ã Fashion-MNIST
classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']


# 4. –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ú–û–î–ï–õ–ï–ô

# 4.1 –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–ê–Ø RESNET34 (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è Fashion-MNIST)
def create_resnet34():
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π ResNet34...")
    model = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)

    # –ó–∞–º–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –¥–ª—è 10 –∫–ª–∞—Å—Å–æ–≤ Fashion-MNIST
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, 10)

    return model


# 4.2 –ö–ê–°–¢–û–ú–ù–ê–Ø CNN (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–∑ –õ–† ‚Ññ1)
class CustomCNN(nn.Module):
    def __init__(self):
        super(CustomCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # –¢—Ä–µ—Ç–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.classifier(x)
        return x


# 5. –§–£–ù–ö–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø (–ü–£–ù–ö–¢ 1)
def train_model(model, train_loader, test_loader, model_name):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam –∏–∑ –õ–† ‚Ññ1

    train_losses = []
    test_accuracies = []

    print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ {model_name}")
    print(f"üìä Batch size: {batch_size}, LR: {learning_rate}")
    print("-" * 50)

    for epoch in range(num_epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        running_loss = 0.0
        start_time = time.time()

        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):
            images, labels = images.to(device), labels.to(device)

            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = model(images)
            loss = criterion(outputs, labels)

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(train_loader)
        accuracy = 100 * correct / total

        train_losses.append(epoch_loss)
        test_accuracies.append(accuracy)

        epoch_time = time.time() - start_time

        print(f"Epoch {epoch + 1}/{num_epochs} | Loss: {epoch_loss:.4f} | "
              f"Accuracy: {accuracy:.2f}% | Time: {epoch_time:.2f}s")

    return train_losses, test_accuracies


# 6. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–ü–£–ù–ö–¢ 1 –ò 2)
def plot_results(resnet_losses, resnet_accs, custom_losses, custom_accs):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    ax1.plot(resnet_losses, 'b-', label='ResNet34', linewidth=2)
    ax1.plot(custom_losses, 'r-', label='Custom CNN', linewidth=2)
    ax1.set_title('Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)

    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    ax2.plot(resnet_accs, 'b-', label='ResNet34', linewidth=2)
    ax2.plot(custom_accs, 'r-', label='Custom CNN', linewidth=2)
    ax2.set_title('Test Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('training_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()


# 7. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô (–ü–£–ù–ö–¢ 4)
def visualize_predictions(model, test_loader, model_name, device):
    model.eval()
    data_iter = iter(test_loader)
    images, labels = next(data_iter)

    with torch.no_grad():
        images = images.to(device)
        outputs = model(images)
        _, predictions = torch.max(outputs, 1)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 8 –ø—Ä–∏–º–µ—Ä–æ–≤
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.ravel()

    for i in range(8):
        if model_name == "ResNet34":
            # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è ResNet
            img = images[i].cpu()
            img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3,
                                                                                                                     1,
                                                                                                                     1)
            img = torch.clamp(img, 0, 1)
            axes[i].imshow(img.permute(1, 2, 0))
        else:
            # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π CNN
            img = images[i].cpu().squeeze()
            img = img * 0.5 + 0.5
            axes[i].imshow(img, cmap='gray')

        color = 'green' if predictions[i] == labels[i] else 'red'
        axes[i].set_title(f'True: {classes[labels[i]]}\nPred: {classes[predictions[i]]}',
                          color=color, fontsize=10)
        axes[i].axis('off')

    plt.suptitle(f'{model_name} - Predictions on Test Images', fontsize=16)
    plt.tight_layout()
    plt.show()


# 8. –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–†–û–ò–ó–í–û–õ–¨–ù–´–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô (–ü–£–ù–ö–¢ 4)
def classify_custom_image(model, image_path, model_name, transform):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if image_path.startswith('http'):
            response = requests.get(image_path)
            img = Image.open(io.BytesIO(response.content))
        else:
            img = Image.open(image_path)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ grayscale –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if img.mode != 'L':
            img = img.convert('L')

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
        input_tensor = transform(img).unsqueeze(0).to(device)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        model.eval()
        with torch.no_grad():
            output = model(input_tensor)
            probabilities = torch.nn.functional.softmax(output[0], dim=0)
            predicted_class = torch.argmax(probabilities).item()
            confidence = probabilities[predicted_class].item()

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        plt.figure(figsize=(10, 4))

        plt.subplot(1, 2, 1)
        plt.imshow(img, cmap='gray')
        plt.title('Input Image')
        plt.axis('off')

        plt.subplot(1, 2, 2)
        class_probs = probabilities.cpu().numpy()
        y_pos = np.arange(len(classes))
        plt.barh(y_pos, class_probs)
        plt.yticks(y_pos, classes)
        plt.xlabel('Probability')
        plt.title(f'{model_name}\nPrediction: {classes[predicted_class]}\nConfidence: {confidence:.2%}')
        plt.tight_layout()
        plt.show()

        return classes[predicted_class], confidence

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return None, 0


# 9. –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø –ò –ê–ù–ê–õ–ò–ó–ê
def main():
    print("=" * 70)
    print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê: –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–´–• –ò –ö–ê–°–¢–û–ú–ù–´–• –°–ï–¢–ï–ô")
    print("=" * 70)

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print("\n1. –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    resnet_model = create_resnet34()
    custom_model = CustomCNN()

    print(f"ResNet34 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in resnet_model.parameters()):,}")
    print(f"Custom CNN –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in custom_model.parameters()):,}")

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (–ü—É–Ω–∫—Ç 1)
    print("\n2. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)

    resnet_losses, resnet_accuracies = train_model(
        resnet_model, train_loader_resnet, test_loader_resnet, "ResNet34"
    )

    custom_losses, custom_accuracies = train_model(
        custom_model, train_loader_custom, test_loader_custom, "Custom CNN"
    )

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ü—É–Ω–∫—Ç 1)
    print("\n3. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø")
    plot_results(resnet_losses, resnet_accuracies, custom_losses, custom_accuracies)

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ü—É–Ω–∫—Ç 2)
    print("\n4. –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 50)
    final_resnet_acc = resnet_accuracies[-1]
    final_custom_acc = custom_accuracies[-1]

    print(f"Final ResNet34 Accuracy: {final_resnet_acc:.2f}%")
    print(f"Final Custom CNN Accuracy: {final_custom_acc:.2f}%")
    print(f"Difference: {final_resnet_acc - final_custom_acc:+.2f}%")

    # –ê–Ω–∞–ª–∏–∑ –∏ State-of-the-art —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (–ü—É–Ω–∫—Ç 3)
    print("\n5. –ê–ù–ê–õ–ò–ó –ò STATE-OF-THE-ART –°–†–ê–í–ù–ï–ù–ò–ï")
    print("=" * 50)

    # State-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è Fashion-MNIST
    sotar_results = {
        "–õ—É—á—à–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã": 96.7,
        "–¢–∏–ø–∏—á–Ω—ã–µ ResNet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã": 93.5,
        "–¢–∏–ø–∏—á–Ω—ã–µ CNN —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã": 92.0,
        "–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏": 89.0
    }

    print("State-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è Fashion-MNIST:")
    for model_type, accuracy in sotar_results.items():
        print(f"  {model_type}: {accuracy}%")

    print(f"\n–ù–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(
        f"  ResNet34: {final_resnet_acc:.2f}% (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {sotar_results['–¢–∏–ø–∏—á–Ω—ã–µ ResNet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'] - final_resnet_acc:.2f}%)")
    print(
        f"  Custom CNN: {final_custom_acc:.2f}% (–æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ: {sotar_results['–¢–∏–ø–∏—á–Ω—ã–µ CNN —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'] - final_custom_acc:.2f}%)")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π (–ü—É–Ω–∫—Ç 4)
    print("\n6. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)

    print("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    visualize_predictions(resnet_model, test_loader_resnet, "ResNet34", device)
    visualize_predictions(custom_model, test_loader_custom, "Custom CNN", device)

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    print("\n7. –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–†–û–ò–ó–í–û–õ–¨–ù–´–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
    print("=" * 50)

    # –ü—Ä–∏–º–µ—Ä—ã URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å–≤–æ–∏)
    test_images = [
        "https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/embedding.gif"
    ]

    print("–ü—Ä–∏–º–µ—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
    for img_url in test_images:
        print(f"\n–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_url}")

        # ResNet34 –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        resnet_pred, resnet_conf = classify_custom_image(
            resnet_model, img_url, "ResNet34", transform_resnet
        )
        if resnet_pred:
            print(f"ResNet34: {resnet_pred} (confidence: {resnet_conf:.2%})")

        # Custom CNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        custom_pred, custom_conf = classify_custom_image(
            custom_model, img_url, "Custom CNN", transform_custom
        )
        if custom_pred:
            print(f"Custom CNN: {custom_pred} (confidence: {custom_conf:.2%})")

    # –í—ã–≤–æ–¥—ã (–ü—É–Ω–∫—Ç 3)
    print("\n8. –í–´–í–û–î–´")
    print("=" * 50)

    print("üìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    print(f"1. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:")
    print(f"   ‚Ä¢ ResNet34 –¥–æ—Å—Ç–∏–≥ —Ç–æ—á–Ω–æ—Å—Ç–∏ {final_resnet_acc:.2f}%")
    print(f"   ‚Ä¢ Custom CNN –¥–æ—Å—Ç–∏–≥ —Ç–æ—á–Ω–æ—Å—Ç–∏ {final_custom_acc:.2f}%")

    print(f"2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä:")
    print(f"   ‚Ä¢ ResNet34 –ø–æ–∫–∞–∑–∞–ª–∞ {'–ª—É—á—à–∏–µ' if final_resnet_acc > final_custom_acc else '—Ö—É–¥—à–∏–µ'} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    print(f"   ‚Ä¢ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {final_resnet_acc - final_custom_acc:+.2f}%")

    print(f"3. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ state-of-the-art:")
    print(
        f"   ‚Ä¢ ResNet34: {final_resnet_acc / sotar_results['–¢–∏–ø–∏—á–Ω—ã–µ ResNet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'] * 100:.1f}% –æ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print(
        f"   ‚Ä¢ Custom CNN: {final_custom_acc / sotar_results['–¢–∏–ø–∏—á–Ω—ã–µ CNN —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'] * 100:.1f}% –æ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    print(f"4. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    if final_resnet_acc > final_custom_acc:
        print("   ‚Ä¢ –î–ª—è Fashion-MNIST –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ")
        print("   ‚Ä¢ –¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–ø—Ä–∞–≤–¥–∞–Ω–æ")
    else:
        print("   ‚Ä¢ –ü—Ä–æ—Å—Ç—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã –¥–ª—è Fashion-MNIST")
        print("   ‚Ä¢ –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä–æ—â–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏ –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–∞—é—Ç—Å—è")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    torch.save(resnet_model.state_dict(), 'resnet34_fashionmnist.pth')
    torch.save(custom_model.state_dict(), 'custom_cnn_fashionmnist.pth')
    print(f"\nüíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: resnet34_fashionmnist.pth, custom_cnn_fashionmnist.pth")


if __name__ == '__main__':
    main()